<!DOCTYPE html>
<!--
	Landed by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>CV4DA - References</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript><link rel="stylesheet" href="assets/css/noscript.css"
    /></noscript>
</head>

<body class="is-preload">
    <div id="page-wrapper">
        <!-- Header -->
        <header id="header">
            <h1 id="logo">
                <a href="index.html">Computer Vision for Driver Assistance</a>
            </h1>
            <nav id="nav">
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li>
                        <a href="technicals.html">Technicals</a>
                        <ul>
                            <li><a href="technicals.html">Introduction</a></li>
                            <li>
                                <a href="related-work.html">Related Work</a>
                                <ul>
                                    <li>
                                        <a href="traffic-sign-recognition.html">Traffic Sign Recognition</a
                      >
                    </li>
                    <li>
                      <a href="pedestrian-detection.html">Pedestrian Detection</a>
                                    </li>
                                    <li>
                                        <a href="vehicle-detection.html">Vehicle Detection</a>
                                    </li>
                                    <li>
                                        <a href="image-captioning.html">Image Captioning</a>
                                    </li>
                                </ul>
                            </li>
                            <li><a href="architecture.html">Architecture</a></li>
                            <!-- <li><a href="Dataset.html">Dataset</a></li> -->
                            <li><a href="sensors.html">Sensors</a></li>
                            <li><a href="success.html">Successes and Failures</a></li>
                            <!-- <li><a href="failure.html">Failures</a></li> -->
                            <li>
                                <a href="challenges-and-future.html">Challenges and Future</a>
                            </li>
                        </ul>
                    </li>
                    <li>
                        <a href="#">Resources</a>
                        <ul>
                            <li>
                                <a href="/docs/project_1_proposal.docx">Project Proposal</a>
                            </li>
                            <li><a href="quiz.html">Quiz</a></li>

                            <li><a href="references.html">References</a></li>
                        </ul>
                    </li>
                    <!-- <li><a href="elements.html">Elements</a></li> -->
                    <!-- <li><a href="#" class="button primary">Sign Up</a></li> -->
                </ul>
            </nav>
        </header>

        <!-- Main -->
        <div id="main" class="wrapper style1">
            <div class="container">
                <header class="major">
                    <h2>References</h2>
                </header>

                <!-- Content -->
                <section id="content">
                    <a href="#" class="image fit"><img src="images/pic07.jpg" alt="" /></a>
                    <ul>
                        [1] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier and S. Lazebnik, "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models," 2015 IEEE International Conference on Computer Vision (ICCV),
                        2015, pp. 2641-2649, doi: 10.1109/ICCV.2015.303. https://ieeexplore.ieee.org/abstract/document/7410660
                    </ul>
                    <ul>
                        [2] Bochkovskiy, Alexey, Chien-Yao Wang and H. Liao. “YOLOv4: Optimal Speed and Accuracy of Object Detection.” ArXiv abs/2004.10934 (2020): n. pag. https://arxiv.org/abs/2004.10934
                    </ul>
                    <ul>
                        [3] Chapel, M. and T. Bouwmans. “Moving Objects Detection with a Moving Camera: A Comprehensive Review.” ArXiv abs/2001.05238 (2020): n. pag. https://arxiv.org/abs/2001.05238
                    </ul>

                    <ul>
                        [4] Divvala, S., Alexei A. Efros and M. Hebert. “How Important Are "Deformable Parts" in the Deformable Parts Model?” ArXiv abs/1206.3714 (2012): n. pag. https://arxiv.org/pdf/1206.3714.pdf
                    </ul>

                    <ul>
                        [5] Girshick, Ross B., Jeff Donahue, Trevor Darrell and J. Malik. “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” 2014 IEEE Conference on Computer Vision and Pattern Recognition (2014): 580-587. https://arxiv.org/abs/1311.2524
                    </ul>

                    <ul>
                        [6] Godard, Clément & Aodha, Oisin & Firman, Michael & Gabriel, Jourdan. (2019). Digging Into Self-Supervised Monocular Depth Estimation. 10.1109/ICCV.2019.00393. https://arxiv.org/abs/1806.01260
                    </ul>
                    <ul>
                        [7] Mohammed, A.S.; Amamou, A.; Ayevide, F.K.; Kelouwani, S.; Agbossou, K.; Zioui, N. The Perception System of Intelligent Ground Vehicles in All Weather Conditions: A Systematic Literature Review. Sensors 2020, 20, 6532. https://doi.org/10.3390/s20226532
                    </ul>
                    <ul>
                        [8] N. Dalal and B. Triggs, “Histograms of oriented gradients for human detection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2005, pp. 886–893. https://ieeexplore.ieee.org/document/1467360
                    </ul>

                    <ul>
                        [9] Papineni, Kishore, S. Roukos, T. Ward and Wei-Jing Zhu. “Bleu: a Method for Automatic Evaluation of Machine Translation.” ACL (2002). https://dl.acm.org/doi/10.3115/1073083.1073135
                    </ul>
                    <ul>
                        [10] P. Dhar, M. Z. Abedin, T. Biswas and A. Datta, "Traffic sign detection — A new approach and recognition using convolution neural network," 2017 IEEE Region 10 Humanitarian Technology Conference (R10-HTC), 2017, pp. 416-419, doi: 10.1109/R10-HTC.2017.8288988.
                        https://ieeexplore.ieee.org/abstract/document/8288988
                    </ul>

                    <ul>
                        [11] Redmon, Joseph & Divvala, Santosh & Girshick, Ross & Farhadi, Ali. (2016). You Only Look Once: Unified, Real-Time Object Detection. 779-788. 10.1109/CVPR.2016.91. https://arxiv.org/abs/1506.02640
                    </ul>

                    <ul>
                        [12] Ren, Shaoqing, Kaiming He, Ross B. Girshick and J. Sun. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” IEEE Transactions on Pattern Analysis and Machine Intelligence 39 (2015): 1137-1149. https://arxiv.org/abs/1506.01497
                    </ul>

                    <ul>
                        [13] Russakovsky, O., Deng, J., Su, H. et al. ImageNet Large Scale Visual Recognition Challenge. Int J Comput Vis 115, 211–252 (2015). https://doi.org/10.1007/s11263-015-0816-y
                    </ul>

                    <ul>
                        [14] Siogkas, George & Skodras, Evangelos. (2012). Traffic Lights Detection in Adverse Conditions Using Color, Symmetry and Spatiotemporal Information. VISAPP 2012 - Proceedings of the International Conference on Computer Vision Theory and Applications.
                        1. https://www.researchgate.net/publication/230601628_Traffic_Lights_Detection_in_Adverse_Conditions_Using_Color_Symmetry_and_Spatiotemporal_Information
                    </ul>

                    <ul>
                        [15] Staudemeyer, Ralf & Morris, Eric. (2019). Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks.
                    </ul>
                    <ul>
                        [16] Vinyals, Oriol & Toshev, Alexander & Bengio, Samy & Erhan, Dumitru. (2016). Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge. IEEE Transactions on Pattern Analysis and Machine Intelligence. 39. 1-1. 10.1109/TPAMI.2016.2587640.
                    </ul>
                    <ul>
                        [17] W. Li, Z. Qu, H. Song, P. Wang and B. Xue, "The Traffic Scene Understanding and Prediction Based on Image Captioning," in IEEE Access, vol. 9, pp. 1420-1427, 2021, doi: 10.1109/ACCESS.2020.3047091.
                    </ul>
                    <ul>
                        [18] Xu, Ke, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, R. Salakhutdinov, R. Zemel and Yoshua Bengio. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” ICML (2015). https://arxiv.org/abs/1502.03044
                    </ul>
                </section>
            </div>
        </div>

        <!-- Footer -->
        <footer id="footer">
            <ul class="icons">
                <!-- <li>
                    <a href="#" class="icon brands alt fa-twitter"><span class="label">Twitter</span></a
            >
          </li> -->
                <!-- <li>
            <a href="#" class="icon brands alt fa-facebook-f"
              ><span class="label">Facebook</span></a
            >
          </li> -->
                <li>
                    <a href="https://www.linkedin.com/in/henry-lao/" class="icon brands alt fa-linkedin-in"><span class="label">LinkedIn</span></a
            >
          </li>
          <!-- <li>
            <a href="#" class="icon brands alt fa-instagram"
              ><span class="label">Instagram</span></a
            >
          </li> -->
          <li>
            <a
              href="https://github.com/henrylao"
              class="icon brands alt fa-github"
              ><span class="label">GitHub</span></a
            >
          </li>
          <li>
            <a
              href="mailto: hlao1995@gmail.com"
              class="icon solid alt fa-envelope"
              ><span class="label">Email</span></a
            >
          </li>
        </ul>
        <ul class="copyright">
          <li>&copy; CV4DA. All rights reserved.</li>
          <li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
            </ul>
        </footer>
    </div>

    <!-- Scripts -->
    <script src="assets/js/jquery.min.js"></script>
    <script src="assets/js/jquery.scrolly.min.js"></script>
    <script src="assets/js/jquery.dropotron.min.js"></script>
    <script src="assets/js/jquery.scrollex.min.js"></script>
    <script src="assets/js/browser.min.js"></script>
    <script src="assets/js/breakpoints.min.js"></script>
    <script src="assets/js/util.js"></script>
    <script src="assets/js/main.js"></script>
</body>

</html>